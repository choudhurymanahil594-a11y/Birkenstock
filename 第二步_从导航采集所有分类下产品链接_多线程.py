# -*- coding: utf-8 -*-
import asyncio
from playwright.async_api import async_playwright
import json
import time
from typing import List, Dict, Tuple
import sys
import os

# æ·»åŠ ä»£ç†æ¨¡å—è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'proxy'))
from proxy import ProxyManager

class ProxyRotator:
    """
    ä»£ç†è½®æ¢å™¨ï¼Œä¸ºæ¯ä¸ªä»»åŠ¡åˆ†é…ä»£ç†
    
    æ³¨æ„ï¼šç”±äºPlaywrightä¸æ”¯æŒå¸¦è®¤è¯çš„SOCKS5ä»£ç†ï¼Œ
    å½“å‰ç‰ˆæœ¬ä¼šè®°å½•ä»£ç†ä¿¡æ¯ä½†ä½¿ç”¨ç›´è¿æ¨¡å¼è¿›è¡Œé‡‡é›†ã€‚
    ä»£ç†ä¿¡æ¯å¯ç”¨äºå…¶ä»–HTTPè¯·æ±‚æˆ–åç»­åŠŸèƒ½æ‰©å±•ã€‚
    """
    
    def __init__(self, proxy_file_path: str = "proxy/working_proxies.json"):
        # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„æ–‡ä»¶è·¯å¾„
        self.proxy_file_path = proxy_file_path
        self.proxy_manager = ProxyManager(proxy_file_path, use_json=True)
        self.working_proxies = []
        self.current_index = 0
        self.load_and_test_proxies()
    
    def load_and_test_proxies(self):
        """åŠ è½½å¹¶æµ‹è¯•æ‰€æœ‰ä»£ç†"""
        print("ğŸ”„ æ­£åœ¨åŠ è½½å¹¶æµ‹è¯•ä»£ç†...")
        print("âš ï¸  æ³¨æ„ï¼šPlaywrightä¸æ”¯æŒå¸¦è®¤è¯çš„SOCKS5ä»£ç†ï¼Œå°†ä½¿ç”¨ç›´è¿æ¨¡å¼")
        
        # é¦–å…ˆå°è¯•ç›´æ¥è¯»å–ä»£ç†æ–‡ä»¶
        try:
            if os.path.exists(self.proxy_file_path):
                with open(self.proxy_file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    raw_proxies = data.get('working_proxies', [])
                    print(f"ğŸ“ ä»æ–‡ä»¶åŠ è½½äº† {len(raw_proxies)} ä¸ªåŸå§‹ä»£ç†")
                    
                    # ç›´æ¥ä½¿ç”¨è¿™äº›ä»£ç†ï¼Œä¸è¿›è¡Œæµ‹è¯•ï¼ˆå› ä¸ºæ–‡ä»¶ä¸­çš„ä»£ç†åº”è¯¥å·²ç»æµ‹è¯•è¿‡äº†ï¼‰
                    self.working_proxies = raw_proxies
                    
                    if self.working_proxies:
                        print(f"âœ… æˆåŠŸåŠ è½½ {len(self.working_proxies)} ä¸ªä»£ç†")
                        print("ğŸ“ ä»£ç†ä¿¡æ¯å°†ç”¨äºä»»åŠ¡åˆ†é…ï¼Œä½†é‡‡é›†æ—¶ä½¿ç”¨ç›´è¿æ¨¡å¼")
                        # æ˜¾ç¤ºä»£ç†ä¿¡æ¯
                        for i, proxy in enumerate(self.working_proxies):
                            print(f"  {i+1}. {proxy['ip']}:{proxy['port']} ({proxy['username']})")
                        return
            else:
                print(f"âš ï¸  ä»£ç†æ–‡ä»¶ä¸å­˜åœ¨: {self.proxy_file_path}")
        except Exception as e:
            print(f"âŒ åŠ è½½ä»£ç†æ–‡ä»¶å¤±è´¥: {e}")
        
        # å¦‚æœç›´æ¥åŠ è½½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ProxyManageræµ‹è¯•
        print("ğŸ”„ å°è¯•ä½¿ç”¨ProxyManageræµ‹è¯•ä»£ç†...")
        try:
            # é‡æ–°åˆå§‹åŒ–ProxyManagerï¼Œä½¿ç”¨ç»å¯¹è·¯å¾„
            abs_path = os.path.abspath(self.proxy_file_path)
            print(f"ğŸ” å°è¯•ä½¿ç”¨ç»å¯¹è·¯å¾„: {abs_path}")
            
            proxy_manager = ProxyManager(abs_path, use_json=True)
            all_proxies = proxy_manager.get_all_proxies()
            print(f"ğŸ“Š ProxyManageråŠ è½½äº† {len(all_proxies)} ä¸ªä»£ç†")
            
            if all_proxies:
                # æµ‹è¯•å‰å‡ ä¸ªä»£ç†
                test_count = min(3, len(all_proxies))
                working_proxies = []
                
                for i in range(test_count):
                    proxy = all_proxies[i]
                    print(f"ğŸ§ª æµ‹è¯•ä»£ç† {i+1}/{test_count}: {proxy['ip']}:{proxy['port']}")
                    if proxy_manager.test_proxy_with_requests(proxy, timeout=5):
                        working_proxies.append(proxy)
                        print(f"  âœ… ä»£ç†å¯ç”¨")
                    else:
                        print(f"  âŒ ä»£ç†ä¸å¯ç”¨")
                
                self.working_proxies = working_proxies
                
                if self.working_proxies:
                    print(f"âœ… æˆåŠŸæµ‹è¯•å¹¶åŠ è½½ {len(self.working_proxies)} ä¸ªå¯ç”¨ä»£ç†")
                    print("ğŸ“ ä»£ç†ä¿¡æ¯å°†ç”¨äºä»»åŠ¡åˆ†é…ï¼Œä½†é‡‡é›†æ—¶ä½¿ç”¨ç›´è¿æ¨¡å¼")
                    return
                else:
                    print("âŒ æ‰€æœ‰æµ‹è¯•çš„ä»£ç†éƒ½ä¸å¯ç”¨")
            else:
                print("âŒ ProxyManageræ²¡æœ‰åŠ è½½åˆ°ä»»ä½•ä»£ç†")
                
        except Exception as e:
            print(f"âŒ ä»£ç†æµ‹è¯•å¤±è´¥: {e}")
        
        print("âš ï¸  å°†ä½¿ç”¨ç›´è¿æ¨¡å¼")
    
    def get_next_proxy(self) -> Dict:
        """è·å–ä¸‹ä¸€ä¸ªä»£ç†"""
        if not self.working_proxies:
            return None
        
        proxy = self.working_proxies[self.current_index]
        self.current_index = (self.current_index + 1) % len(self.working_proxies)
        return proxy
    
    def get_proxy_count(self) -> int:
        """è·å–å¯ç”¨ä»£ç†æ•°é‡"""
        return len(self.working_proxies)

async def scrape_product_urls_from_category(browser, category_data, semaphore, proxy_info=None):
    """
    ä»å•ä¸ªåˆ†ç±»é¡µé¢ä¸Šé‡‡é›†æ‰€æœ‰äº§å“çš„URLã€‚
    """
    async with semaphore:  # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
        url = category_data['level3_url']
        all_product_urls = []
        context = None
        page = None
        
        try:
            # åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼Œå¦‚æœæä¾›äº†ä»£ç†åˆ™ä½¿ç”¨ä»£ç†
            if proxy_info:
                # Playwrightä¸æ”¯æŒå¸¦è®¤è¯çš„SOCKS5ä»£ç†ï¼Œæ‰€ä»¥ä½¿ç”¨ç›´è¿æ¨¡å¼
                # ä½†æˆ‘ä»¬å¯ä»¥è®°å½•ä»£ç†ä¿¡æ¯ç”¨äºå…¶ä»–ç”¨é€”
                print(f"  [{category_data['level3_category']}] ä»£ç†ä¿¡æ¯: {proxy_info['ip']}:{proxy_info['port']} (Playwrightä¸æ”¯æŒå¸¦è®¤è¯çš„SOCKS5ä»£ç†ï¼Œä½¿ç”¨ç›´è¿æ¨¡å¼)")
                context = await browser.new_context()
            else:
                context = await browser.new_context()
                print(f"  [{category_data['level3_category']}] ä½¿ç”¨ç›´è¿æ¨¡å¼")
            
            page = await context.new_page()
            print(f"æ­£åœ¨å¤„ç†: {category_data['level3_category']} - {url}")
            
            await page.goto(url, timeout=60000)
            await page.wait_for_load_state('domcontentloaded')

            # å¾ªç¯ç‚¹å‡»"åŠ è½½æ›´å¤š"æŒ‰é’®ï¼Œç›´åˆ°æ‰€æœ‰äº§å“éƒ½åŠ è½½å®Œæ¯•
            load_more_count = 0
            while True:
                # å°è¯•æŸ¥æ‰¾"åŠ è½½æ›´å¤š"æŒ‰é’®
                load_more_button_locator = page.locator('button.button-custom-black.outline')
                
                # æ£€æŸ¥æŒ‰é’®æ˜¯å¦å­˜åœ¨ä¸”å¯è§
                if await load_more_button_locator.is_visible():
                    load_more_count += 1
                    print(f"  [{category_data['level3_category']}] ç¬¬{load_more_count}æ¬¡ç‚¹å‡» 'åŠ è½½æ›´å¤š' æŒ‰é’®...")
                    try:
                        await load_more_button_locator.click(timeout=5000)
                        # ç­‰å¾…æ–°äº§å“åŠ è½½
                        await page.wait_for_timeout(2000)
                    except Exception as click_e:
                        print(f"  [{category_data['level3_category']}] ç‚¹å‡» 'åŠ è½½æ›´å¤š' æŒ‰é’®å¤±è´¥: {click_e}")
                        break
                else:
                    if load_more_count > 0:
                        print(f"  [{category_data['level3_category']}] å·²åŠ è½½å®Œæ¯•ï¼Œå…±ç‚¹å‡»äº† {load_more_count} æ¬¡")
                    break

            # æå–æ‰€æœ‰äº§å“é“¾æ¥
            product_tile_elements = await page.query_selector_all('a.product-tile')
            if not product_tile_elements:
                print(f"  [{category_data['level3_category']}] æœªæ‰¾åˆ°ä»»ä½•äº§å“é“¾æ¥")
            else:
                for element in product_tile_elements:
                    href = await element.get_attribute('href')
                    if href:
                        # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
                        if href.startswith('/'):
                            full_url = f'https://www.birkenstock.com{href}'
                        else:
                            full_url = href
                        all_product_urls.append(full_url)
                
                print(f"  [{category_data['level3_category']}] æˆåŠŸé‡‡é›†åˆ° {len(all_product_urls)} ä¸ªäº§å“URL")
                
        except Exception as e:
            print(f"  [{category_data['level3_category']}] é‡‡é›†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        finally:
            if page:
                await page.close()
            if context:
                await context.close()
        
        return all_product_urls, category_data

async def main():
    """
    ä¸»å‡½æ•°ï¼Œä½¿ç”¨å¼‚æ­¥5çº¿ç¨‹å¤„ç†ï¼Œé›†æˆä»£ç†åŠŸèƒ½
    """
    start_time = time.time()
    
    # åˆå§‹åŒ–ä»£ç†è½®æ¢å™¨
    print("ğŸš€ åˆå§‹åŒ–ä»£ç†ç³»ç»Ÿ...")
    proxy_rotator = ProxyRotator()
    
    # æ­¥éª¤ 1: ä»JSONæ–‡ä»¶ä¸­è¯»å–åˆ†ç±»ä¿¡æ¯
    try:
        with open('ç¬¬ä¸€æ­¥_å¯¼èˆªç›®å½•.json', 'r', encoding='utf-8') as f:
            categories_data = json.load(f)
    except FileNotFoundError:
        print("é”™è¯¯: æœªæ‰¾åˆ° ç¬¬ä¸€æ­¥_å¯¼èˆªç›®å½•.json æ–‡ä»¶ã€‚")
        return
    except json.JSONDecodeError:
        print("é”™è¯¯: è§£æ ç¬¬ä¸€æ­¥_å¯¼èˆªç›®å½•.json æ–‡ä»¶å¤±è´¥ã€‚")
        return

    # æ­¥éª¤ 2: æŸ¥æ‰¾æ‰€æœ‰ä¸‰çº§åˆ†ç±»ä¿¡æ¯
    third_level_categories_to_scrape = []
    for level1_cat in categories_data:
        level1_category = level1_cat.get('level1_category')
        level1_url = level1_cat.get('level1_url')
        for level2_cat in level1_cat.get('children', []):
            level2_category = level2_cat.get('level2_category')
            level2_url = level2_cat.get('level2_url')
            for level3_cat in level2_cat.get('children', []):
                if 'level3_url' in level3_cat and 'level3_category' in level3_cat:
                    third_level_categories_to_scrape.append({
                        "level1_category": level1_category,
                        "level1_url": level1_url,
                        "level2_category": level2_category,
                        "level2_url": level2_url,
                        "level3_category": level3_cat['level3_category'],
                        "level3_url": level3_cat['level3_url'],
                        "product_urls": []
                    })
    
    if not third_level_categories_to_scrape:
        print("æœªæ‰¾åˆ°ä»»ä½•ä¸‰çº§åˆ†ç±»ã€‚")
        return

    print(f"æ‰¾åˆ° {len(third_level_categories_to_scrape)} ä¸ªä¸‰çº§åˆ†ç±»è¿›è¡Œé‡‡é›†ã€‚")
    print("å¼€å§‹å¼‚æ­¥5çº¿ç¨‹å¤„ç†...")

    # æ­¥éª¤ 3: ä½¿ç”¨å¼‚æ­¥5çº¿ç¨‹é‡‡é›†äº§å“URL
    total_product_urls_count = 0
    urls_without_products = []
    processed_categories = []
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        
        # è®¾ç½®5ä¸ªå¹¶å‘çº¿ç¨‹
        concurrency_limit = 10
        semaphore = asyncio.Semaphore(concurrency_limit)
        
        # å°†åˆ†ç±»åˆ†æˆæ‰¹æ¬¡å¤„ç†ï¼Œæ¯æ‰¹5ä¸ª
        batch_size = concurrency_limit
        category_batches = [
            third_level_categories_to_scrape[i:i + batch_size] 
            for i in range(0, len(third_level_categories_to_scrape), batch_size)
        ]
        
        print(f"å°† {len(third_level_categories_to_scrape)} ä¸ªåˆ†ç±»åˆ†æˆ {len(category_batches)} æ‰¹å¤„ç†")
        
        # é€æ‰¹å¤„ç†ï¼Œæ¯æ‰¹5ä¸ªå¹¶å‘
        for batch_index, batch in enumerate(category_batches, 1):
            print(f"\næ­£åœ¨å¤„ç†ç¬¬ {batch_index}/{len(category_batches)} æ‰¹ ({len(batch)} ä¸ªåˆ†ç±»)...")
            
            # åˆ›å»ºå½“å‰æ‰¹æ¬¡çš„å¼‚æ­¥ä»»åŠ¡
            batch_tasks = []
            for category_data in batch:
                task = scrape_product_urls_from_category(browser, category_data, semaphore, proxy_rotator.get_next_proxy())
                batch_tasks.append(task)
            
            # ç­‰å¾…å½“å‰æ‰¹æ¬¡å®Œæˆ
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
            
            # å¤„ç†å½“å‰æ‰¹æ¬¡çš„ç»“æœ
            for result in batch_results:
                if isinstance(result, Exception):
                    print(f"æ‰¹æ¬¡å¤„ç†ä¸­å‘ç”Ÿé”™è¯¯: {result}")
                    continue
                    
                scraped_urls, original_category_data = result
                original_category_data['product_urls'] = scraped_urls
                processed_categories.append(original_category_data)
                total_product_urls_count += len(scraped_urls)
                
                if not scraped_urls:
                    urls_without_products.append(original_category_data['level3_url'])
                else:
                    print(f"âœ“ {original_category_data['level3_category']}: {len(scraped_urls)} ä¸ªäº§å“URL")
            
            # æ‰¹æ¬¡é—´çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…è¿‡åº¦è¯·æ±‚
            if batch_index < len(category_batches):
                await asyncio.sleep(1)
        
        await browser.close()

    # æ­¥éª¤ 4: ä¿å­˜ç»“æœ
    elapsed_time = time.time() - start_time
    print(f"\né‡‡é›†å®Œæˆï¼")
    print(f"æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
    print(f"æ€»å…±é‡‡é›†åˆ° {total_product_urls_count} ä¸ªäº§å“URL")
    print(f"æˆåŠŸå¤„ç† {len(processed_categories)} ä¸ªåˆ†ç±»")
    print(f"æœªæ‰¾åˆ°äº§å“çš„åˆ†ç±»: {len(urls_without_products)} ä¸ª")
    print(f"ä½¿ç”¨çš„ä»£ç†æ•°é‡: {proxy_rotator.get_proxy_count()}")

    # ä¿å­˜åŒ…å«äº§å“URLçš„åˆ†ç±»æ•°æ®
    with open('ç¬¬äºŒæ­¥_äº§å“é“¾æ¥.json', 'w', encoding='utf-8') as f:
        json.dump(processed_categories, f, ensure_ascii=False, indent=4)
    print("äº§å“é“¾æ¥æ•°æ®å·²ä¿å­˜åˆ° ç¬¬äºŒæ­¥_äº§å“é“¾æ¥.json")

    # ä¿å­˜æœªæ‰¾åˆ°äº§å“çš„URL
    if urls_without_products:
        with open('ç¬¬äºŒæ­¥_æœªæ‰¾åˆ°ä»»ä½•äº§å“.json', 'w', encoding='utf-8') as f:
            json.dump(urls_without_products, f, ensure_ascii=False, indent=4)
        print("æœªæ‰¾åˆ°äº§å“çš„URLå·²ä¿å­˜åˆ° ç¬¬äºŒæ­¥_æœªæ‰¾åˆ°ä»»ä½•äº§å“.json")

if __name__ == "__main__":
    asyncio.run(main())
